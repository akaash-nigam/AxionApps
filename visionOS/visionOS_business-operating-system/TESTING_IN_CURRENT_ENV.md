# Testing That Can Be Done in Current Environment

This document describes all tests that can be executed **without** Xcode, visionOS Simulator, or Vision Pro hardware.

## Overview

While the Swift visionOS application requires Xcode for testing, we can validate all backend components, infrastructure, and documentation in this Linux environment.

---

## 1. API Specification Validation

### What We're Testing
- OpenAPI 3.0 specification syntax
- Schema completeness
- Endpoint consistency
- Response format validation

### Test Script: `tests/api/validate_openapi.sh`

```bash
#!/bin/bash
set -e

echo "=== OpenAPI Specification Validation ==="

# Install validator if needed
if ! command -v openapi-validator &> /dev/null; then
    echo "Installing OpenAPI validator..."
    npm install -g @ibm-cloud/openapi-ruleset spectral-cli
fi

# Validate OpenAPI spec
echo "Validating api-specification.yaml..."
spectral lint api-specification.yaml --ruleset spectral:oas

echo "âœ… OpenAPI specification is valid"

# Check for required fields
echo "Checking for required components..."

required_paths=(
    "/auth/login"
    "/auth/logout"
    "/organization"
    "/departments"
    "/kpis"
    "/ai/analyze-anomaly"
)

for path in "${required_paths[@]}"; do
    if grep -q "$path:" api-specification.yaml; then
        echo "âœ“ Found endpoint: $path"
    else
        echo "âœ— Missing endpoint: $path"
        exit 1
    fi
done

echo "âœ… All required endpoints present"
```

### Run It
```bash
cd /home/user/visionOS_business-operating-system
chmod +x tests/api/validate_openapi.sh
./tests/api/validate_openapi.sh
```

---

## 2. Database Schema Validation

### What We're Testing
- SQL syntax correctness
- Table relationships
- Index definitions
- Constraint validation

### Test Script: `tests/database/validate_schema.py`

```python
#!/usr/bin/env python3
"""
Database Schema Validation
Tests PostgreSQL schema without requiring a running database
"""

import re
import sys

def validate_sql_file(filepath):
    """Validate SQL schema file"""
    print(f"=== Validating {filepath} ===\n")

    with open(filepath, 'r') as f:
        content = f.read()

    # Track findings
    tables = []
    indexes = []
    foreign_keys = []
    errors = []

    # Find CREATE TABLE statements
    table_pattern = r'CREATE TABLE\s+(\w+)\s*\('
    tables = re.findall(table_pattern, content, re.IGNORECASE)
    print(f"âœ“ Found {len(tables)} tables: {', '.join(tables)}\n")

    # Expected tables
    expected_tables = [
        'organizations', 'users', 'departments', 'employees',
        'kpis', 'reports', 'collaboration_sessions', 'audit_logs',
        'kpi_history', 'analytics_events', 'notifications'
    ]

    for table in expected_tables:
        if table in tables:
            print(f"âœ“ Table '{table}' defined")
        else:
            errors.append(f"âœ— Missing required table: {table}")

    print()

    # Find indexes
    index_pattern = r'CREATE INDEX\s+(\w+)'
    indexes = re.findall(index_pattern, content, re.IGNORECASE)
    print(f"âœ“ Found {len(indexes)} indexes\n")

    # Find foreign keys
    fk_pattern = r'FOREIGN KEY\s*\([^)]+\)\s*REFERENCES\s+(\w+)'
    foreign_keys = re.findall(fk_pattern, content, re.IGNORECASE)
    print(f"âœ“ Found {len(foreign_keys)} foreign key relationships\n")

    # Check for RLS policies
    if 'ROW LEVEL SECURITY' in content:
        print("âœ“ Row Level Security policies defined\n")
    else:
        errors.append("âœ— No Row Level Security policies found")

    # Check for partitioning
    if 'PARTITION BY' in content:
        partition_pattern = r'(\w+).*PARTITION BY'
        partitioned_tables = re.findall(partition_pattern, content, re.IGNORECASE)
        print(f"âœ“ Found {len(partitioned_tables)} partitioned tables: {', '.join(partitioned_tables)}\n")

    # Check for triggers
    if 'CREATE TRIGGER' in content or 'CREATE OR REPLACE FUNCTION' in content:
        print("âœ“ Database triggers/functions defined\n")

    # Print errors
    if errors:
        print("\nâš ï¸  WARNINGS:")
        for error in errors:
            print(f"  {error}")
        print()

    # Summary
    print("=" * 50)
    print(f"SUMMARY:")
    print(f"  Tables: {len(tables)}")
    print(f"  Indexes: {len(indexes)}")
    print(f"  Foreign Keys: {len(foreign_keys)}")
    print(f"  Errors: {len(errors)}")
    print("=" * 50)

    if errors:
        return False

    print("\nâœ… Schema validation passed!")
    return True

if __name__ == "__main__":
    filepath = "database-schema.sql"
    success = validate_sql_file(filepath)
    sys.exit(0 if success else 1)
```

### Run It
```bash
python3 tests/database/validate_schema.py
```

---

## 3. WebSocket Protocol Documentation Validation

### What We're Testing
- Message type completeness
- Schema consistency
- Example validity

### Test Script: `tests/websocket/validate_protocol.py`

```python
#!/usr/bin/env python3
"""
WebSocket Protocol Validation
Ensures protocol documentation is complete and consistent
"""

import json
import re

def validate_protocol_doc(filepath):
    """Validate WebSocket protocol documentation"""
    print(f"=== Validating {filepath} ===\n")

    with open(filepath, 'r') as f:
        content = f.read()

    # Expected message types from the spec
    expected_message_types = [
        'AUTH', 'AUTH_SUCCESS', 'AUTH_FAILED',
        'SUBSCRIBE', 'UNSUBSCRIBE',
        'KPI_UPDATED', 'DEPARTMENT_UPDATED', 'EMPLOYEE_UPDATED',
        'SESSION_CREATED', 'SESSION_JOINED', 'SESSION_LEFT',
        'CURSOR_MOVE', 'SELECTION_CHANGE', 'COMMENT_ADDED',
        'ERROR', 'HEARTBEAT', 'PONG'
    ]

    found_types = []
    missing_types = []

    for msg_type in expected_message_types:
        # Check if message type is documented
        if msg_type in content:
            found_types.append(msg_type)
            print(f"âœ“ Message type '{msg_type}' documented")
        else:
            missing_types.append(msg_type)
            print(f"âœ— Message type '{msg_type}' not found")

    print(f"\nâœ“ Found {len(found_types)}/{len(expected_message_types)} message types\n")

    # Check for Swift code examples
    if '```swift' in content:
        swift_examples = content.count('```swift')
        print(f"âœ“ Found {swift_examples} Swift code examples\n")
    else:
        print("âš ï¸  No Swift code examples found\n")

    # Check for JSON examples
    if '```json' in content:
        json_examples = content.count('```json')
        print(f"âœ“ Found {json_examples} JSON examples\n")
    else:
        print("âš ï¸  No JSON examples found\n")

    # Summary
    print("=" * 50)
    print(f"SUMMARY:")
    print(f"  Message types documented: {len(found_types)}/{len(expected_message_types)}")
    print(f"  Missing: {len(missing_types)}")
    print("=" * 50)

    if missing_types:
        print(f"\nâš ï¸  Missing message types: {', '.join(missing_types)}")
        return False

    print("\nâœ… Protocol documentation validation passed!")
    return True

if __name__ == "__main__":
    success = validate_protocol_doc("WEBSOCKET_PROTOCOL.md")
    exit(0 if success else 1)
```

### Run It
```bash
python3 tests/websocket/validate_protocol.py
```

---

## 4. Landing Page Quality Tests

### What We're Testing
- HTML validity
- CSS syntax
- JavaScript functionality
- Performance metrics
- Accessibility

### Test Script: `tests/landing-page/validate_page.sh`

```bash
#!/bin/bash
set -e

echo "=== Landing Page Validation ==="

cd landing-page-v2

# Check file sizes (performance)
echo "Checking file sizes..."
html_size=$(wc -c < index.html)
css_size=$(wc -c < css/styles.css)
js_size=$(wc -c < js/main.js)

echo "  index.html: $(($html_size / 1024))KB"
echo "  styles.css: $(($css_size / 1024))KB"
echo "  main.js: $(($js_size / 1024))KB"

total_size=$(($html_size + $css_size + $js_size))
echo "  Total: $(($total_size / 1024))KB"

if [ $total_size -lt 200000 ]; then
    echo "âœ“ Total size under 200KB (excellent!)"
else
    echo "âš ï¸  Total size over 200KB"
fi

echo ""

# Check for critical elements
echo "Checking for critical elements..."

critical_elements=(
    "<!DOCTYPE html>"
    "<title>"
    "viewport"
    "description"
    "ROI Calculator"
    "pricing"
    "features"
)

for element in "${critical_elements[@]}"; do
    if grep -q "$element" index.html; then
        echo "âœ“ Found: $element"
    else
        echo "âœ— Missing: $element"
    fi
done

echo ""

# Check CSS
echo "Checking CSS..."
if grep -q "@media" css/styles.css; then
    media_queries=$(grep -c "@media" css/styles.css)
    echo "âœ“ Found $media_queries media queries (responsive design)"
else
    echo "âš ï¸  No media queries found"
fi

if grep -q "animation" css/styles.css; then
    animations=$(grep -c "@keyframes" css/styles.css)
    echo "âœ“ Found $animations animations"
fi

echo ""

# Check JavaScript
echo "Checking JavaScript..."
if grep -q "calculateROI" js/main.js; then
    echo "âœ“ ROI calculator function present"
fi

if grep -q "addEventListener" js/main.js; then
    echo "âœ“ Event listeners present"
fi

if grep -q "IntersectionObserver" js/main.js; then
    echo "âœ“ Intersection Observer for animations"
fi

echo ""
echo "âœ… Landing page validation complete!"
```

### Run It
```bash
chmod +x tests/landing-page/validate_page.sh
./tests/landing-page/validate_page.sh
```

---

## 5. CI/CD Workflow Validation

### What We're Testing
- GitHub Actions syntax
- Workflow completeness
- Job dependencies

### Test Script: `tests/cicd/validate_workflows.sh`

```bash
#!/bin/bash
set -e

echo "=== CI/CD Workflow Validation ==="

# Check if workflows exist
workflows=(
    ".github/workflows/ci.yml"
    ".github/workflows/deploy.yml"
    ".github/workflows/pr-automation.yml"
)

for workflow in "${workflows[@]}"; do
    if [ -f "$workflow" ]; then
        echo "âœ“ Found: $workflow"

        # Check for required fields
        if grep -q "name:" "$workflow" && \
           grep -q "on:" "$workflow" && \
           grep -q "jobs:" "$workflow"; then
            echo "  âœ“ Has required fields (name, on, jobs)"
        else
            echo "  âœ— Missing required fields"
        fi

        # Count jobs
        jobs=$(grep -c "^  [a-zA-Z_-]*:" "$workflow" || true)
        echo "  âœ“ Contains $jobs jobs"

    else
        echo "âœ— Missing: $workflow"
    fi
    echo ""
done

echo "âœ… Workflow validation complete!"
```

### Run It
```bash
chmod +x tests/cicd/validate_workflows.sh
./tests/cicd/validate_workflows.sh
```

---

## 6. Documentation Completeness Check

### Test Script: `tests/docs/validate_docs.sh`

```bash
#!/bin/bash
set -e

echo "=== Documentation Validation ==="

# Required documentation files
required_docs=(
    "README.md"
    "IMPLEMENTATION_PLAN.md"
    "FEATURE_COMPLETION_ANALYSIS.md"
    "ADDITIONAL_DELIVERABLES_GUIDE.md"
    "api-specification.yaml"
    "database-schema.sql"
    "DATABASE_ERD.md"
    "WEBSOCKET_PROTOCOL.md"
    "TODO_VISIONOS_ENV.md"
    "TESTING_IN_CURRENT_ENV.md"
)

echo "Checking for required documentation files..."
echo ""

missing_count=0
for doc in "${required_docs[@]}"; do
    if [ -f "$doc" ]; then
        # Get file size
        size=$(wc -c < "$doc")
        lines=$(wc -l < "$doc")
        echo "âœ“ $doc ($(($size / 1024))KB, $lines lines)"
    else
        echo "âœ— Missing: $doc"
        ((missing_count++))
    fi
done

echo ""
echo "=" * 50
echo "Documentation files: $((${#required_docs[@]} - $missing_count))/${#required_docs[@]}"

if [ $missing_count -eq 0 ]; then
    echo "âœ… All documentation present!"
else
    echo "âš ï¸  $missing_count files missing"
fi
```

### Run It
```bash
chmod +x tests/docs/validate_docs.sh
./tests/docs/validate_docs.sh
```

---

## 7. Mock Data Validation

### Test Script: `tests/data/validate_mocks.py`

```python
#!/usr/bin/env python3
"""
Validate mock datasets for testing
"""

import json
import sys
from pathlib import Path

def validate_mock_data():
    """Validate all mock data files"""
    print("=== Mock Data Validation ===\n")

    mock_files = {
        'tests/data/mock-organizations.json': validate_organizations,
        'tests/data/mock-departments.json': validate_departments,
        'tests/data/mock-employees.json': validate_employees,
        'tests/data/mock-kpis.json': validate_kpis,
    }

    all_valid = True

    for filepath, validator in mock_files.items():
        if Path(filepath).exists():
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                if validator(data):
                    print(f"âœ“ {filepath} is valid")
                else:
                    print(f"âœ— {filepath} has validation errors")
                    all_valid = False
            except json.JSONDecodeError as e:
                print(f"âœ— {filepath} has invalid JSON: {e}")
                all_valid = False
        else:
            print(f"âš ï¸  {filepath} not found (will be created)")

    print()
    return all_valid

def validate_organizations(data):
    """Validate organization mock data"""
    required_fields = ['id', 'name', 'structure', 'created_at']
    for org in data:
        for field in required_fields:
            if field not in org:
                print(f"  âœ— Missing field '{field}' in organization")
                return False
    return True

def validate_departments(data):
    """Validate department mock data"""
    required_fields = ['id', 'name', 'organization_id', 'budget_allocated']
    for dept in data:
        for field in required_fields:
            if field not in dept:
                print(f"  âœ— Missing field '{field}' in department")
                return False
    return True

def validate_employees(data):
    """Validate employee mock data"""
    required_fields = ['id', 'name', 'email', 'department_id', 'role']
    for emp in data:
        for field in required_fields:
            if field not in emp:
                print(f"  âœ— Missing field '{field}' in employee")
                return False
    return True

def validate_kpis(data):
    """Validate KPI mock data"""
    required_fields = ['id', 'name', 'value', 'target', 'department_id']
    for kpi in data:
        for field in required_fields:
            if field not in kpi:
                print(f"  âœ— Missing field '{field}' in KPI")
                return False
    return True

if __name__ == "__main__":
    success = validate_mock_data()
    if success:
        print("âœ… Mock data validation passed!")
        sys.exit(0)
    else:
        print("âš ï¸  Mock data validation had warnings")
        sys.exit(0)  # Don't fail, just warn
```

### Run It
```bash
python3 tests/data/validate_mocks.py
```

---

## Running All Tests

### Master Test Script: `run-all-tests.sh`

```bash
#!/bin/bash

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘   Business Operating System - Current Environment Tests   â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track results
total_tests=0
passed_tests=0
failed_tests=0

run_test() {
    local test_name=$1
    local test_command=$2

    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "Running: $test_name"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

    ((total_tests++))

    if $test_command; then
        echo "âœ… PASSED: $test_name"
        ((passed_tests++))
    else
        echo "âŒ FAILED: $test_name"
        ((failed_tests++))
    fi

    echo ""
}

# Create test directories
mkdir -p tests/{api,database,websocket,landing-page,cicd,docs,data}

# Run all tests
run_test "Documentation Validation" "bash tests/docs/validate_docs.sh"
run_test "Database Schema Validation" "python3 tests/database/validate_schema.py"
run_test "WebSocket Protocol Validation" "python3 tests/websocket/validate_protocol.py"
run_test "Landing Page Validation" "bash tests/landing-page/validate_page.sh"
run_test "CI/CD Workflows Validation" "bash tests/cicd/validate_workflows.sh"
run_test "Mock Data Validation" "python3 tests/data/validate_mocks.py"

# Note: OpenAPI validation requires npm package, skip if not installed
if command -v spectral &> /dev/null; then
    run_test "OpenAPI Specification Validation" "bash tests/api/validate_openapi.sh"
else
    echo "â­ï¸  Skipping OpenAPI validation (spectral-cli not installed)"
    echo "   Install with: npm install -g @stoplight/spectral-cli"
    echo ""
fi

# Print summary
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                      TEST SUMMARY                          â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Total Tests:  $total_tests"
echo "Passed:       $passed_tests âœ…"
echo "Failed:       $failed_tests âŒ"
echo ""

if [ $failed_tests -eq 0 ]; then
    echo "ğŸ‰ All tests passed!"
    exit 0
else
    echo "âš ï¸  Some tests failed. Please review the output above."
    exit 1
fi
```

### Run All Tests
```bash
chmod +x run-all-tests.sh
./run-all-tests.sh
```

---

## Test Results Storage

All test results should be stored in `test-results/` directory:

```
test-results/
â”œâ”€â”€ latest-run.log
â”œâ”€â”€ api-validation.log
â”œâ”€â”€ database-validation.log
â”œâ”€â”€ websocket-validation.log
â”œâ”€â”€ landing-page-validation.log
â””â”€â”€ summary.txt
```

---

## Continuous Integration

These tests are automatically run by GitHub Actions on every push. See `.github/workflows/ci.yml` for configuration.

---

## What Can't Be Tested Here

The following require Xcode/visionOS environment (see `TODO_VISIONOS_ENV.md`):

- âŒ Swift unit tests (XCTest)
- âŒ SwiftUI view tests
- âŒ RealityKit 3D rendering
- âŒ ViewModel integration tests
- âŒ API client integration (needs running backend)
- âŒ WebSocket client (needs running WebSocket server)
- âŒ Performance profiling
- âŒ Vision Pro device testing

---

## Next Steps

1. âœ… Run all tests in this environment: `./run-all-tests.sh`
2. âœ… Review any failures and fix issues
3. âœ… Commit test results
4. â­ï¸  Move to Xcode environment for Swift development (see `TODO_VISIONOS_ENV.md`)

---

## Questions?

- See `TODO_VISIONOS_ENV.md` for tasks requiring Xcode
- See `IMPLEMENTATION_PLAN.md` for overall development roadmap
- See `FEATURE_COMPLETION_ANALYSIS.md` for current completion status
